{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install using \"pip install surprise\"\n",
    "import numpy as np\n",
    "import numba\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix factorization functions from homework\n",
    "\n",
    "@numba.jit\n",
    "def grad_U(Ui, Yij, Vj, reg, eta):\n",
    "    \"\"\"\n",
    "    Takes as input Ui (the ith row of U), a training point Yij, the column\n",
    "    vector Vj (jth column of V^T), reg (the regularization parameter lambda),\n",
    "    and eta (the learning rate).\n",
    "\n",
    "    Returns the gradient of the regularized loss function with\n",
    "    respect to Ui multiplied by eta.\n",
    "    \"\"\"\n",
    "    # Gradient with respect to Ui\n",
    "    return eta*(reg*Ui-Vj*(Yij-np.dot(Ui.T,Vj)))\n",
    "\n",
    "@numba.jit\n",
    "def grad_V(Vj, Yij, Ui, reg, eta):\n",
    "    \"\"\"\n",
    "    Takes as input the column vector Vj (jth column of V^T), a training point Yij,\n",
    "    Ui (the ith row of U), reg (the regularization parameter lambda),\n",
    "    and eta (the learning rate).\n",
    "\n",
    "    Returns the gradient of the regularized loss function with\n",
    "    respect to Vj multiplied by eta.\n",
    "    \"\"\"\n",
    "    # Gradient with respect to Vj\n",
    "    return eta*(reg*Vj-Ui*(Yij-np.dot(Vj.T,Ui)))\n",
    "\n",
    "@numba.jit\n",
    "def get_err(U, V, Y, reg=0.0):\n",
    "    \"\"\"\n",
    "    Takes as input a matrix Y of triples (i, j, Y_ij) where i is the index of a user,\n",
    "    j is the index of a movie, and Y_ij is user i's rating of movie j and\n",
    "    user/movie matrices U and V.\n",
    "\n",
    "    Returns the mean regularized squared-error of predictions made by\n",
    "    estimating Y_{ij} as the dot product of the ith row of U and the jth column of V^T.\n",
    "    \"\"\"\n",
    "    err = 0\n",
    "    # Compute squared error\n",
    "    for i,j,Yij in Y:\n",
    "        i=i-1\n",
    "        j=j-1\n",
    "        err = err + (Yij-np.dot(U[i],V[j]))**2\n",
    "    # Return MSE + regularization\n",
    "    return err/len(Y)/2+reg/2.0*(np.linalg.norm(U)**2+np.linalg.norm(V)**2)\n",
    "\n",
    "@numba.jit\n",
    "def train_model(M, N, K, eta, reg, Y, eps=0.0001, max_epochs=300):\n",
    "    \"\"\"\n",
    "    Given a training data matrix Y containing rows (i, j, Y_ij)\n",
    "    where Y_ij is user i's rating on movie j, learns an\n",
    "    M x K matrix U and N x K matrix V such that rating Y_ij is approximated\n",
    "    by (UV^T)_ij.\n",
    "\n",
    "    Uses a learning rate of <eta> and regularization of <reg>. Stops after\n",
    "    <max_epochs> epochs, or once the magnitude of the decrease in regularized\n",
    "    MSE between epochs is smaller than a fraction <eps> of the decrease in\n",
    "    MSE after the first epoch.\n",
    "\n",
    "    Returns a tuple (U, V, err) consisting of U, V, and the unregularized MSE\n",
    "    of the model.\n",
    "    \"\"\"\n",
    "    # initial U and V to random floats from -0.5 to 0.5\n",
    "    U = np.reshape(np.random.rand(M*K)-0.5,(M,K))\n",
    "    V = np.reshape(np.random.rand(N*K)-0.5,(N,K))\n",
    "    # Storage of U,V,err for each epoch\n",
    "    data = []\n",
    "    # For eps termination\n",
    "    preverr = 100\n",
    "    for epoch in range(0,max_epochs):\n",
    "        # Shuffles the data before each epoch\n",
    "        idx=np.random.permutation(np.arange(len(Y)))\n",
    "        for i,j,Yij in Y[idx]:\n",
    "            # Convert indices counting from 0\n",
    "            i=i-1\n",
    "            j=j-1\n",
    "            Ui = U[i]\n",
    "            Vj = V[j]\n",
    "            # Randomly update U or V\n",
    "            if np.random.randint(2):                \n",
    "                U[i]=U[i]-grad_U(Ui, Yij, Vj, reg, eta)\n",
    "            else:\n",
    "                V[j]=V[j]-grad_V(Vj, Yij, Ui, reg, eta)\n",
    "        # Compute the error\n",
    "        err = get_err(U,V,Y)\n",
    "        data.append([U,V,err])\n",
    "        if epoch%5==0:\n",
    "            print('SGD on epoch ',epoch,', error=',err)\n",
    "        # Terminate if error reduction is less than eps\n",
    "        if abs(err-preverr) < eps:\n",
    "            print('EPS condition reached')\n",
    "            break\n",
    "        preverr = err\n",
    "    # Final output\n",
    "    data=np.array(data)\n",
    "    return data[-1,0],data[-1,1],data[-1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reader that reads tab-separated data\n",
    "full = np.loadtxt(\"./data/data.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factorizing with  943  users,  1682  movies.\n",
      "SGD on epoch  0 , error= 0.8293882299754082\n",
      "SGD on epoch  5 , error= 0.4125036687598385\n",
      "SGD on epoch  10 , error= 0.3714642873395004\n",
      "SGD on epoch  15 , error= 0.35067387237350633\n",
      "SGD on epoch  20 , error= 0.3348051133854553\n",
      "SGD on epoch  25 , error= 0.319366314666794\n",
      "SGD on epoch  30 , error= 0.3086820632406794\n",
      "SGD on epoch  35 , error= 0.3028175514745843\n",
      "SGD on epoch  40 , error= 0.2960228035720193\n",
      "SGD on epoch  45 , error= 0.29206103756773055\n",
      "SGD on epoch  50 , error= 0.2853965463047724\n",
      "SGD on epoch  55 , error= 0.2868181514681604\n",
      "SGD on epoch  60 , error= 0.2862695741037143\n",
      "SGD on epoch  65 , error= 0.2839851960489585\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Test out the latent model\n",
    "Y_train = np.loadtxt('./data/train.txt').astype(int)\n",
    "Y_test = np.loadtxt('./data/test.txt').astype(int)\n",
    "\n",
    "M = max(max(Y_train[:,0]), max(Y_test[:,0])).astype(int) # users\n",
    "N = max(max(Y_train[:,1]), max(Y_test[:,1])).astype(int) # movies\n",
    "print(\"Factorizing with \", M, \" users, \", N, \" movies.\")\n",
    "Ks = [10,20,30,50,100]\n",
    "\n",
    "reg = 0.1\n",
    "eta = 0.03 # learning rate\n",
    "U,V,err = train_model(M, N, Ks[1], eta, reg, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on Full Dataset, Getting Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVD and Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do SVD (singular value decomposition) on the matrices\n",
    "A, Sigma, B = np.linalg.svd(V, full_matrices=False) # V = A @ np.diag(Sigma) @ B \n",
    "print(V.shape)\n",
    "print(A.shape)\n",
    "print(np.diag(Sigma).shape)\n",
    "print(B.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the first 2 rows(?) of A\n",
    "A_red = A[:2, :]\n",
    "print(A_red.shape)\n",
    "\n",
    "# Multiply with V to reduce V from 20 dim to 2 dim\n",
    "V_red = (V @ A_red.T)\n",
    "print(V_red.shape)\n",
    "print(V_red)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter the 2 dimensions of the reduced V\n",
    "plt.scatter(V_red[:, 0], V_red[:, 1])\n",
    "\n",
    "# Top 10 most popular movies\n",
    "for i in [50, 258, 100, 181, 294, 286, 288, 1, 300, 121]:\n",
    "    plt.scatter(V_red[clf.trainset.to_inner_iid(str(i)), 0], V_red[clf.trainset.to_inner_iid(str(i)), 1], c='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Copy the code from the basic visualization that allowed us sort the movies so that we can subselect the relevant\n",
    "# subsets of the movies.\n",
    "for i in [50, 258, 100, 181, 294, 286, 288, 1, 300, 121]:\n",
    "    plt.scatter(V_red[clf.trainset.to_inner_iid(str(i)), 0], V_red[clf.trainset.to_inner_iid(str(i)), 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
